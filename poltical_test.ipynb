{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if a general purpose sentiment analysis model can be used to predict the sentiment of political tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClassifier(\n",
       "  (bert): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import RobertaModel\n",
    "from transformers import RobertaTokenizer\n",
    "from roberta_classifer import RobertaClassifier\n",
    "    \n",
    "# Instantiate the model\n",
    "model = RobertaClassifier()\n",
    "\n",
    "# Load pre-trained weights\n",
    "#model.load_state_dict(torch.load('fine_tuned_roberta_classifier.pt'))\n",
    "\n",
    "model.to('cuda')\n",
    "# Set the model to evaluation mode\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_dataset = pd.read_csv('data/pro-posts.csv')\n",
    "against_dataset = pd.read_csv('data/against-posts.csv')\n",
    "#Combine the datasets\n",
    "dataset = pd.concat([pro_dataset, against_dataset], ignore_index=True)\n",
    "#Shuffle the dataset\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['text'], dataset['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RobertaClassifier.preprocess_for_roberta() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_inputs, train_masks \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpreprocess_for_roberta(x_train)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone training masking.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m test_inputs, test_masks \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpreprocess_for_roberta(x_test)\n",
      "\u001b[1;31mTypeError\u001b[0m: RobertaClassifier.preprocess_for_roberta() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks = model.preprocess_for_roberta(x_train)\n",
    "print('Done training masking.')\n",
    "test_inputs, test_masks = model.preprocess_for_roberta(x_test)\n",
    "print('Done testing masking.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 16\n",
    "\n",
    "# Turn labels into a Tensor\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_dataloader = model.create_train_dataloader(train_inputs, train_masks, y_train, batch_size)\n",
    "\n",
    "# Create the DataLoader for our testing set\n",
    "test_dataloader = model.create_test_dataloader(test_inputs, test_masks, y_test, batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "# Loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "random.seed(20)\n",
    "np.random.seed(20)\n",
    "torch.manual_seed(20)\n",
    "torch.cuda.manual_seed_all(20)\n",
    "\n",
    "def train(roberta_classifer, optimizer, scheduler, epochs=4):\n",
    "    loss_hist = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        roberta_classifer.train()\n",
    "\n",
    "        progress_bar = tqdm(total=len(train_dataloader), desc=f'Epoch {epoch+1}', position=0)\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "\n",
    "            # Load batch to GPU\n",
    "            batch_inputs, batch_masks, batch_labels = tuple(t.to('cuda') for t in batch)\n",
    "\n",
    "            # Zero out gradients\n",
    "            roberta_classifer.zero_grad()\n",
    "\n",
    "            # Perform a forward pass.\n",
    "            logits = roberta_classifer(batch_inputs, batch_masks)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Perform a backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            \n",
    "            # Clip norm\n",
    "            torch.nn.utils.clip_grad_norm_(roberta_classifer.parameters(), 1.0)\n",
    "\n",
    "            # step optimizer, update params\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({'Elapsed': time.time() - t0_epoch, 'Loss': total_loss / batch_counts})\n",
    "        \n",
    "        # Calculate the average loss\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        loss_hist.append(avg_train_loss)\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "# Loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "random.seed(20)\n",
    "np.random.seed(20)\n",
    "torch.manual_seed(20)\n",
    "torch.cuda.manual_seed_all(20)\n",
    "\n",
    "def train(roberta_classifier, optimizer, scheduler, epochs=4):\n",
    "    loss_hist = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        roberta_classifier.train()\n",
    "\n",
    "        progress_bar = tqdm(total=len(train_dataloader), desc=f'Epoch {epoch+1}', position=0)\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "\n",
    "            # Load batch to GPU\n",
    "            batch_inputs, batch_masks, batch_labels = tuple(t.to('cuda') for t in batch)\n",
    "\n",
    "            # Zero out gradients\n",
    "            roberta_classifier.zero_grad()\n",
    "\n",
    "            # Perform a forward pass.\n",
    "            logits = roberta_classifier(batch_inputs, batch_masks)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Perform a backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            \n",
    "            # Clip norm\n",
    "            torch.nn.utils.clip_grad_norm_(roberta_classifier.parameters(), 1.0)\n",
    "\n",
    "            # step optimizer, update params\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Progress update every 20 batches.\n",
    "            # if step % 20 == 0 and not step == 0:\n",
    "            #     # Calculate elapsed time in minutes.\n",
    "            #     elapsed = time.time() - t0_batch\n",
    "\n",
    "            #     # Print training results\n",
    "            #     print(f\"{epoch:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {elapsed:^9.2f}\")\n",
    "\n",
    "            #     # Reset batch tracking variables\n",
    "            #     batch_loss, batch_counts = 0, 0\n",
    "            #     t0_batch = time.time()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({'Elapsed': time.time() - t0_epoch, 'Loss': total_loss / batch_counts})\n",
    "        \n",
    "        # Calculate the average loss\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        loss_hist.append(avg_train_loss)\n",
    "        print(\"-\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "\n",
    "\n",
    "train(model, model.get_optimizer(), model.get_scheduler, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    figure = plt.figure()\n",
    "    plt.title(f'Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.annotate(f'Acc: {accuracy*100:.2f}%', xy=(0.8, 0.2))\n",
    "    plt.show()\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def roberta_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to('cuda') for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate confidence\n",
    "    probs = F.softmax(all_logits, dim=1).cpu()\n",
    "\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "test_inputs, test_masks = model.preprocess_for_roberta(test_data)\n",
    "\n",
    "# Create dataloader\n",
    "test_dataloader = model.create_dataloader(test_inputs, test_masks, batch_size)\n",
    "\n",
    "# Compute predicted probabilities on the test set\n",
    "probs = torch.argmax(roberta_predict(model, test_dataloader), dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "\n",
    "def create_prediction_dictionary(sentences, labels):\n",
    "    prediction_dict = {}\n",
    "    for i in range(len(sentences)):\n",
    "        prediction_dict[sentences[i]] = labels_map[labels[i].item()]\n",
    "    return prediction_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = create_prediction_dictionary(test_data, probs)\n",
    "sentiment_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facebook-scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d814e548a47325464e0881685482a82a9f7ebc33e0749d2240cc066ce05521f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
