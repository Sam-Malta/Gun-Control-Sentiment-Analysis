{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if a general purpose sentiment analysis model can be used to predict the sentiment of political tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import RobertaModel\n",
    "from transformers import RobertaTokenizer\n",
    "from roberta_classifer import RobertaClassifier\n",
    "    \n",
    "# Instantiate the model\n",
    "model = RobertaClassifier()\n",
    "\n",
    "# Load pre-trained weights\n",
    "#model.load_state_dict(torch.load('model_weights.pt'))\n",
    "\n",
    "model.to('cuda')\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = 'data'\n",
    "\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f'Loading : {file_name}')\n",
    "        datasets.append(pd.read_csv(file_path))\n",
    "\n",
    "#Combine the datasets\n",
    "dataset = pd.concat(datasets, ignore_index=True)\n",
    "#Shuffle the dataset\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#Remove the unnecessary columns\n",
    "dataset = dataset.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "dataset.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['text'], dataset['label'], test_size=0.2, random_state=42)\n",
    "x_train.reset_index(drop=True, inplace=True)\n",
    "x_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_masks = model.preprocess_for_roberta(x_train)\n",
    "print('Done training masking.')\n",
    "test_inputs, test_masks = model.preprocess_for_roberta(x_test)\n",
    "print('Done testing masking.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 13\n",
    "\n",
    "# Turn labels into a Tensor\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_dataloader = model.create_train_dataloader(train_inputs, train_masks, y_train, batch_size)\n",
    "\n",
    "# Create the DataLoader for our testing set\n",
    "test_dataloader = model.create_test_dataloader(test_inputs, test_masks, batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "# Loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "random.seed(20)\n",
    "np.random.seed(20)\n",
    "torch.manual_seed(20)\n",
    "torch.cuda.manual_seed_all(20)\n",
    "\n",
    "def train(roberta_classifier, optimizer, scheduler, epochs=4):\n",
    "    loss_hist = []\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        roberta_classifier.train()\n",
    "\n",
    "        progress_bar = tqdm(total=len(train_dataloader), desc=f'Epoch {epoch+1}', position=0)\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "\n",
    "            # Load batch to GPU\n",
    "            batch_inputs, batch_masks, batch_labels = tuple(t.to('cuda') for t in batch)\n",
    "            # Zero out gradients\n",
    "            roberta_classifier.zero_grad()\n",
    "\n",
    "            # Perform a forward pass.\n",
    "            logits = roberta_classifier(batch_inputs, batch_masks)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Perform a backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            \n",
    "            # Clip norm\n",
    "            torch.nn.utils.clip_grad_norm_(roberta_classifier.parameters(), 1.0)\n",
    "\n",
    "            # step optimizer, update params\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Progress update every 20 batches.\n",
    "            # if step % 20 == 0 and not step == 0:\n",
    "            #     # Calculate elapsed time in minutes.\n",
    "            #     elapsed = time.time() - t0_batch\n",
    "\n",
    "            #     # Print training results\n",
    "            #     print(f\"{epoch:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {elapsed:^9.2f}\")\n",
    "\n",
    "            #     # Reset batch tracking variables\n",
    "            #     batch_loss, batch_counts = 0, 0\n",
    "            #     t0_batch = time.time()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({'Elapsed': time.time() - t0_epoch, 'Loss': total_loss / batch_counts})\n",
    "        \n",
    "        # Calculate the average loss\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        loss_hist.append(avg_train_loss)\n",
    "        print(\"-\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "\n",
    "optimizer = model.get_optimizer(lr=5e-5)\n",
    "train(model, optimizer, model.get_scheduler(optimizer, train_dataloader, epochs=num_epochs), epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train == 0).sum(), (y_train == 1).sum(), (y_train == 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC (multiclass ROC)\n",
    "    - Create and plot the confusion matrix as percentages\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), num_classes)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    num_classes = probs.shape[1]\n",
    "    \n",
    "    # One-hot encode the true labels\n",
    "    y_binary = label_binarize(y_true, classes=np.arange(num_classes))\n",
    "    \n",
    "    # Initialize dictionaries to store ROC curves and AUC values for each class\n",
    "    roc_curves = {}\n",
    "    auc_values = {}\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_binary[:, i], probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_values[i] = roc_auc\n",
    "        \n",
    "        # Plot ROC curve for each class\n",
    "        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.annotate(f'Acc: {accuracy*100:.2f}%', xy=(0.8, 0.2))\n",
    "    plt.show()\n",
    "    \n",
    "    # Create and plot the confusion matrix as percentages\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_names = np.arange(num_classes)  # Replace with your class labels\n",
    "    \n",
    "    # Calculate percentages for each cell\n",
    "    cm_percent = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]) * 100\n",
    "    \n",
    "    confusion_df = pd.DataFrame(cm_percent, index=class_names, columns=class_names)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_df, annot=True, cmap='Blues', fmt='.2f', cbar=False)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix (Percentages)')\n",
    "    plt.show()\n",
    "    \n",
    "    return auc_values, accuracy\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'probs' contains the predicted probabilities with shape (len(y_test), num_classes)\n",
    "# and 'y_test' contains the true labels\n",
    "# auc_values, accuracy = evaluate_roc(probs, y_test)\n",
    "\n",
    "\n",
    "def evaluate_roc_binary(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    figure = plt.figure()\n",
    "    plt.title(f'Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.annotate(f'Acc: {accuracy*100:.2f}%', xy=(0.8, 0.2))\n",
    "    plt.show()\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def roberta_predict(loader):\n",
    "    \"\"\"Perform a forward pass on the trained Roberta model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "\n",
    "    all_logits = []\n",
    "    model.eval()\n",
    "    # For each batch in our test set...\n",
    "    for batch in loader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to('cuda') for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate confidence\n",
    "    probs = F.softmax(all_logits, dim=1).cpu()\n",
    "\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def evaluate_f1(preds, truth, threshold=0.5):\n",
    "    # Convert the input tensors to NumPy arrays for consistency\n",
    "    preds = preds.cpu().numpy()\n",
    "    truth = truth.cpu().numpy()\n",
    "\n",
    "    # Apply the threshold to convert probabilities to binary labels\n",
    "    binary_preds = (preds > threshold).astype(int)\n",
    "\n",
    "    # Calculate the F1 score for each label and average them (macro-average)\n",
    "    f1_scores = f1_score(truth, binary_preds, average='macro')\n",
    "\n",
    "    # Calculate the F1 score for each label individually\n",
    "    f1_scores_per_label = f1_score(truth, binary_preds, average=None)\n",
    "\n",
    "    # Plot the F1 scores for each label\n",
    "    labels = np.arange(len(f1_scores_per_label))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(labels, f1_scores_per_label)\n",
    "    plt.xticks(labels)\n",
    "    plt.xlabel('Label Index')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score per Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return f1_scores, f1_scores_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = roberta_predict(test_dataloader)\n",
    "evaluate_roc(preds, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "torch.save(model.state_dict(), 'model_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prompt(prompt):\n",
    "\n",
    "    model.eval()\n",
    "    prompt = [prompt]\n",
    "    prompt_inputs, prompt_masks = model.preprocess_for_roberta(prompt)\n",
    "    prompt_inputs = torch.tensor(prompt_inputs)\n",
    "    prompt_masks = torch.tensor(prompt_masks)\n",
    "    prompt_dataloader = model.create_test_dataloader(prompt_inputs, prompt_masks, 1)\n",
    "    probs = roberta_predict(prompt_dataloader)\n",
    "    print(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = input(\"Enter a prompt: \")\n",
    "print(predict_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on a Larger Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('comment/moms_demand.csv')\n",
    "comments = comments.dropna(subset=['comment_text'])\n",
    "batch_size = 16\n",
    "comment_inputs, comment_masks = model.preprocess_for_roberta(comments['comment_text'])\n",
    "\n",
    "# Create dataloader\n",
    "comment_dataloader = model.create_test_dataloader(comment_inputs, comment_masks, batch_size=batch_size)\n",
    "\n",
    "# Compute predicted probabilities on the test set\n",
    "probs = torch.argmax(roberta_predict(comment_dataloader), dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "color=[\"c\", \"purple\", \"gray\"]\n",
    "values = [torch.sum(probs == 0), torch.sum(probs == 1), torch.sum(probs == 2)]\n",
    "ax1.bar([\"Anti-Gun Control\", \"Pro-Gun Control\", \"Unrelated\"], values, color=color)\n",
    "ax1.set_title(\"Gun Control Sentiment on a NRA Post Regarding New Gun Control Laws\")\n",
    "ax1.set_ylabel(\"Number of Comments\")\n",
    "ax1.set_xlabel(\"Sentiment\")\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "for i, v in enumerate(values):\n",
    "    ax1.text(i, v + 2, int(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a pie chart\n",
    "\n",
    "ax2.pie(values, colors=color, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Display the pie chart\n",
    "plt.show()\n",
    "fig.savefig('parkland_nra.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facebook-scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d814e548a47325464e0881685482a82a9f7ebc33e0749d2240cc066ce05521f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
