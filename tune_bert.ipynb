{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140 Dataset\n",
    "\n",
    "The Sentiment140 dataset consists of 1,600,000 tweets collected using the Twitter API. These tweets have been annotated for sentiment, with the polarity labeled as 0 (negative), 2 (neutral), or 4 (positive). The dataset is commonly used for sentiment analysis tasks and sentiment classification.\n",
    "\n",
    "The dataset contains the following six fields:\n",
    "\n",
    "- **target**: The polarity of the tweet, where 0 represents negative sentiment, 2 represents neutral sentiment, and 4 represents positive sentiment.\n",
    "- **ids**: The unique identifier of the tweet.\n",
    "- **date**: The date and time the tweet was posted in UTC format.\n",
    "- **flag**: The query associated with the tweet. If there is no query, the value is labeled as NO_QUERY.\n",
    "- **user**: The username of the Twitter user who posted the tweet.\n",
    "- **text**: The actual text content of the tweet.\n",
    "\n",
    "This dataset was created for sentiment analysis research and can be used to train models or develop algorithms to detect sentiment in text data, specifically tweets.\n",
    "\n",
    "### Key Points\n",
    "- 1,600,000 tweets with sentiment annotations (0 = negative, 2 = neutral, 4 = positive).\n",
    "- Extracted using the Twitter API.\n",
    "- Fields include target, ids, date, flag, user, and text.\n",
    "- Used for sentiment analysis and sentiment classification tasks.\n",
    "- Valuable resource for training models and developing sentiment analysis algorithms.\n",
    "\n",
    "Please note that this dataset was originally introduced in a research project conducted at Stanford University, and the official link and paper are provided for further reference and citation.\n",
    "\n",
    "### Acknowledgements\n",
    "- [Kaggle Dataset Link](https://www.kaggle.com/kazanova/sentiment140)\n",
    "\n",
    "### Reference/Citation\n",
    "Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "#!pip install matplotlib\n",
    "#!pip install transformers\n",
    "#!pip install torch #Make sure to install the correct version of torch for your system (GPU or CPU)\n",
    "#!pip install scikit-learn\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install tqdm\n",
    "#!pip install nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = pd.read_csv(\"dataset.csv\", encoding=\"latin-1\", engine='pyarrow')\n",
    "original_dataset.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
    "original_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process of fine-tuning the BERT model for sentiment analysis, it is not necessary to include certain columns such as date, username, query, and id. We can focus solely on utilizing the text and target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = original_dataset.drop([\"time\", \"date\", \"query\", \"username\"], axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = dataset['label'].value_counts()\n",
    "plt.bar(value_counts.index, value_counts.values)\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Values')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the description of the dataset states that the target column contains values of 0, 2, and 4, the actual values in the dataset are 0 and 4. This means that there are only two classes of sentiment in the dataset: negative and positive. We will transform the values in the target column to 0 and 1 to make it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the labels from 4 to 1\n",
    "dataset['label'] = dataset['label'].replace(4, 1)\n",
    "value_counts = dataset['label'].value_counts()\n",
    "plt.bar(value_counts.index, value_counts.values)\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Values')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning BERT\n",
    "\n",
    "Now that we have a better understanding of the dataset, we can begin fine-tuning the BERT model for sentiment analysis. We will use the transformers library by Hugging Face to fine-tune the pretrained BERT model for sentiment classification. The transformers library provides a wide variety of pretrained transformer models along with the necessary methods and classes to fine-tune language models to suit a given NLP task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we fine-tune\n",
    "\n",
    "We need to preprocess the text to remove any special characters and @user mentions. We will also split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['text'], dataset['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "del dataset # free up some memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because BERT was trained on full sentences, we don't have to remove stop words or punctiation. This text however does contain links, and other errors that we must fix. We will use the transformers library to tokenize and encode the sentences in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    # Remove entity mentions starting with '@user'\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'(http|https)://[^\\s]+', '', text)\n",
    "    \n",
    "    # Remove text between two asterisks -- Usually emojis/symbols in the dataset\n",
    "    text = re.sub(r'\\*.*?\\*', '', text)\n",
    "    \n",
    "    # Correct common errors\n",
    "    text = text.replace('&amp;', '&')\n",
    "    \n",
    "    # Remove excess whitespace characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.apply(lambda text: text_preprocessing(text))\n",
    "x_test = x_test.apply(lambda text: text_preprocessing(text))\n",
    "x_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a pre-trained BERT model effectively, you need to use the provided tokenizer because the model has a fixed vocabulary, and the tokenizer handles out-of-vocabulary words. Special tokens should be added at the start and end of each sentence, sentences should be padded or truncated to a consistent length, and an attention mask is used to identify padding tokens. We will use the transformer library to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_bert(data):\n",
    "    # Initialize lists to store the input_ids and attention_masks\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    for text in tqdm(data):\n",
    "        # Use tokenizer.encode_plus to tokenize and encode the text\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,\n",
    "            max_length=360, # Determined from the processed text\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # Add the input_ids and attention_mask to the lists\n",
    "        input_ids.append(encoded.get('input_ids'))\n",
    "        attention_masks.append(encoded.get('attention_mask'))\n",
    "\n",
    "    # Convert the lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "\n",
    "    return input_ids, attention_masks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing will take a while because of how large the dataset is\n",
    "For me it was ~ 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_inputs, train_masks = preprocess_for_bert(x_train)\n",
    "print('Done training preprocessing.')\n",
    "test_inputs, test_masks = preprocess_for_bert(x_test)\n",
    "print('Done testing preprocessing.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original: ', x_train[0])\n",
    "print('Tokenized: ', train_masks[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can create a Data Loader from Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert the labels to tensors\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "test_labels = torch.tensor(y_test.values)\n",
    "\n",
    "# Batch Size (Usually 16 or 32)\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our testing set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, we can fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Optimizer to Fine Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def init_model(epochs=4):\n",
    "    bert_classifier = BertClassifier(freeze=False)\n",
    "\n",
    "    # Run GPU\n",
    "\n",
    "    bert_classifier.to('cuda')\n",
    "\n",
    "    optimizer = AdamW(bert_classifier.parameters(), lr=5e-5)\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "# Loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "random.seed(20)\n",
    "np.random.seed(20)\n",
    "torch.manual_seed(20)\n",
    "torch.cuda.manual_seed_all(20)\n",
    "\n",
    "def train(bert_classifier, optimizer, scheduler, epochs=4):\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        bert_classifier.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "\n",
    "            # Load batch to GPU\n",
    "            batch_inputs, batch_masks, batch_labels = tuple(t.to('cuda') for t in batch)\n",
    "\n",
    "            # Zero out gradients\n",
    "            bert_classifier.zero_grad()\n",
    "\n",
    "            # Perform a forward pass.\n",
    "            logits = bert_classifier(batch_inputs, batch_masks)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip norm\n",
    "            torch.nn.utils.clip_grad_norm_(bert_classifier.parameters(), 1.0)\n",
    "\n",
    "            # step optimizer, update params\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Progress update every 20 batches.\n",
    "            if step % 20 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "        \n",
    "        # Calculate the average loss\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(bert_classifier, dataloader):\n",
    "\n",
    "    #Set to eval mode\n",
    "    bert_classifier.eval()\n",
    "\n",
    "    #Tracking\n",
    "    val_acc = []\n",
    "    val_loss = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        #Load to GPU\n",
    "        batch_inputs, batch_masks, batch_labels = tuple(t.to('cuda') for t in batch)\n",
    "\n",
    "        #Forward pass\n",
    "        with torch.no_grad():\n",
    "            logits = bert_classifier(batch_inputs, batch_masks)\n",
    "\n",
    "        #Compute loss\n",
    "        loss = loss_fn(logits, batch_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        #Get predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        #Calculate accuracy\n",
    "        accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "        val_acc.append(accuracy)\n",
    "\n",
    "    #Compute the average accuracy and loss over the validation set\n",
    "    val_acc = np.mean(val_acc)\n",
    "    val_loss = np.mean(val_loss)\n",
    "\n",
    "    return val_acc, val_loss\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifer, optimizer, scheduler = init_model(epochs=10)\n",
    "train(bert_classifer, optimizer, scheduler, epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
